// import natural from 'natural'

// let tokenizer = new natural.WordTokenizer
// let wordnet = new natural.WordNet()
// const stemmer = natural.PorterStemmer;
// const SentimentAnalyzer = natural.SentimentAnalyzer;
// const analyzer = new SentimentAnalyzer('English', stemmer, 'afinn');
// const documentPath = 'personal_content.txt';
// const text = readFileSync(documentPath, 'utf-8');
// const question = 'How do you stay updated on the latest trends and developments in the marketing technology landscape? '






// import { readFileSync, writeFileSync } from 'fs'
// import tf from '@tensorflow/tfjs-node'
// import use from '@tensorflow-models/universal-sentence-encoder'

// // Path to the file containing the text data

// const documentPath = 'personal_content.txt';
// const text = readFileSync(documentPath, 'utf-8');

// async function createEmbeddings(text) {
//   const model = await use.load();
//   const embeddings = await model.embed(text)
//   return embeddings.arraySync()
// }

// async function saveEmbeddingsToJSON(embeddings, sentences, filename) {
//   const json = {};
//   for (let i = 0; i < sentences.length; i++) {
//     json[sentences[i]] = embeddings[i];
//   }
//   const jsonString = JSON.stringify(json);
//   writeFileSync(filename, jsonString);
// }
// // const embeddings = await createEmbeddings(text);
// const filename = './embeddings.json';
// // await saveEmbeddingsToJSON(embeddings, text, filename);
// const readContextEmbeddings = (file) => {
//   const jsonString = readFileSync(filename)
//   const json = JSON.parse(jsonString)
//   const embeddings = Object.values(json)
//   return embeddings[0]
// }
// const contextEmbeddings = readContextEmbeddings(filename)
// // const queryEmbeddings = await createEmbeddings(question)

// const context = async (query) => {
//   function cosineSimilarity(a, b) {
//     b = [b]
//     const dotProduct = a.reduce((acc, val) => acc + val * b[0], 0);
//     const magnitudeA = Math.sqrt(a.reduce((acc, val) => acc + val ** 2, 0));
//     const magnitudeB = Math.sqrt(b.reduce((acc, val) => acc + val ** 2, 0));
//     return dotProduct / (magnitudeA * magnitudeB);
//   }

//   const k = 1000;
//   const queryEmbeddings = await createEmbeddings(query)
//   const similarities = [];
//   for (let i = 0; i < contextEmbeddings.length; i++) {
//     const similarity = cosineSimilarity(queryEmbeddings[0], contextEmbeddings[i])
//     if (Math.abs(similarity) > 0.9) {
//       similarities.push({ index: i, similarity: similarity})
//     }
//     console.log('length: ', similarities.length)
//   }
//   if (similarities.length !== 0) {
//       similarities.sort((a,b) => b.similarity - a.similarity)
//       const indices = similarities.slice(0,k).map(s => s.index)
//       const sentences = text.split(/[.?!]/g).map(s => s.trim()).filter(Boolean)
//       console.log(similarities)
//       const selectedSentences = indices.map(i => sentences[i])
//       // console.log(selectedSentences.join('. '))
//       return selectedSentences.join('. ')
//   }

//   }
  

// export default context





// const paragraphs = text.split('\n').map(paragraph => paragraph.trim()).filter(Boolean);
// const sentences = []
// for (let i = 0; i < paragraphs.length; i++) {
//   const sentence = paragraphs[i].split('.')
//   sentences.push(sentence)
// }
// console.log(paragraphs.length)
// async function createEmbeddings(text) {
//   const model = await use.load();
//   const embeddings = await model.embed(text)
//   return embeddings.arraySync()
// }

// async function saveEmbeddingsToJSON(paragraphs, filename) {
//   const json = {};
//   const paragraphEmbeddings = []
//   for (let i = 0; 1 < paragraphs.length; i++) {
//     const sentenceEmbeddings = await createEmbeddings(paragraphs[i].split('. '));
//     console.log(i)
//     const paragraphEmbedding = tf.mean(sentenceEmbeddings, 0).arraySync()
//     paragraphEmbeddings.push(paragraphEmbedding)
//     console.log('ok')
//     json[paragraphs[i]] = paragraphEmbeddings[i];
//   }

//   // for (let i = 0; i < paragraphs.length; i++) {
//   //   json[paragraphs[i]] = paragraphEmbeddings[i];
//   // }
//   const jsonString = JSON.stringify(json);
//   writeFileSync(filename, jsonString);
// }
// // // const paragraphEmbeddings = await createEmbeddings(paragraphs);
// const filename = './embeddings.json';
// await saveEmbeddingsToJSON(paragraphs, filename);

// const pathToDocx = 'personal_content.docx';

// // Extract text and images from the Word document
// const images = await mammoth.convertToHtml({ path: pathToDocx })
//   .then((result) => {
//     const html = result.value; // The extracted HTML content
//     const messages = result.messages; // Any messages or warnings from Mammoth

//     // Find and extract the images from the HTML content
//     const imageTags = html.match(/<img[^>]+>/g) || [];
//     const images = imageTags.map((tag) => {
//       const src = tag.match(/src="([^"]*)"/)[1];
//       // const alt = tag.match(/alt="([^"]*)"/)[1];
//      return { src };
//     });
//     return images

//   })
//   .catch((error) => {
//     console.error('Error extracting content from Word document:', error);
//   });
//   const text = await mammoth.extractRawText({ path: pathToDocx })
//   .then((result) => {
//     const text = result.value; // The extracted plain text
//     const messages = result.messages; // Any messages or warnings from Mammoth
//     return text
//   })
//   .catch((error) => {
//     console.error('Error extracting content from Word document:', error);
//   });
